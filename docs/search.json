[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Monkey Banana Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSolving the monkey banana problem with monte carlo methods\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the monkey banana problem with monte carlo methods\n\n\nMonte carlo code for the Monkey Banana problem\n\n\n\n\n\nMay 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the monkey banana problem with dynamic programming\n\n\nDynamic programming code for the Monkey Banana problem\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the monkey banana problem with dynamic programming\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Machines that Learn and Think Like People\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n‘Insight’ in the pigeon: antecedents and determinants of an intelligent performance\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/problem-context/index.html",
    "href": "posts/problem-context/index.html",
    "title": "Monkey Banana Problem Context",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/ideas/index.html",
    "href": "posts/ideas/index.html",
    "title": "The Pigeon Banana Problem",
    "section": "",
    "text": "Consider this video of a pigeon solving the monkey banana problem (or rather, the pigeon banana problem). You can see that there is some element of exploration - it climbs on top of the box, pecks at it, ascertains that the box’s position can be moved. It also tries, in vain, to get at the banana, but soon realizes that it is out of reach.\nAt one point it seems to make a mental connection and starts pushing the box towards the banana. Can we say that an action plan spontaneously formed in the brain of the pigeon? Was this plan-forming possible due to the pigeon’s ability to compose actions it had previously been trained on?\nThe paper describes the specific training procedure used to get the pigeon to do this. What’s fascinating is that the subtasks never included the full end-to-end task. In other words, the pigeon in the video is solving a novel problem, somehow composing things it has “learned”.\nHere are the two main subtasks that the pigeon was trained on:\n\nDirectional pushing - the pigeon needs to push a box towards a green spot until the box covers the spot.\nClimbing and pecking - the pigeon needs to climb a box positioned under the banana and peck it."
  },
  {
    "objectID": "posts/a-classical-planning-approach/index.html",
    "href": "posts/a-classical-planning-approach/index.html",
    "title": "A Classical Planning Approach",
    "section": "",
    "text": "wefwe"
  },
  {
    "objectID": "posts/the-pigeon-banana-problem/index.html",
    "href": "posts/the-pigeon-banana-problem/index.html",
    "title": "‘Insight’ in the pigeon: antecedents and determinants of an intelligent performance",
    "section": "",
    "text": "Consider this video of a pigeon solving the monkey banana problem (or rather, the pigeon banana problem). You can see that there is some element of exploration - it climbs on top of the box, pecks at it, ascertains that the box’s position can be moved. It also tries, in vain, to get at the banana, but soon realizes that it is out of reach.\nAt one point it seems to make a mental connection and starts pushing the box towards the banana. Can we say that an action plan spontaneously formed in the brain of the pigeon? Was this plan-forming possible due to the pigeon’s ability to compose actions it had previously been trained on?\nThe paper describes the specific training procedure used to get the pigeon to do this. What’s fascinating is that the subtasks never included the full end-to-end task. In other words, the pigeon in the video is solving a novel problem, somehow composing things it has “learned”.\nHere are the two main subtasks that the pigeon was trained on:\n\nDirectional pushing - the pigeon needs to push a box towards a green spot until the box covers the spot.\nClimbing and pecking - the pigeon needs to climb a box positioned under the banana and peck it."
  },
  {
    "objectID": "posts/monkey-banana-mdp/monkey-banana-mdp.html",
    "href": "posts/monkey-banana-mdp/monkey-banana-mdp.html",
    "title": "Solving Monkey Banana with Dynamic Programming",
    "section": "",
    "text": "We will create a simplified Monkey Banana environment to test classical RL methods on.\nThe environment is a “2D world” with discrete and finite states, actions, and rewards. It is fully observable to our agent, the Monkey.\n\nState\nState consists of 5 values:\n\nagent position (x-axis)\nchair position (x-axis)\nbanana position (x-axis)\nis_holding_chair (0 or 1)\non_chair (0 or 1)\n\nAs you can see, we do not explicitly model the y-axis, since all we care about is whether the Monkey is on the chair when he reaches for the banana.\n\n\nActions\n\nmove left one step\nmove right one step\nclimb on the chair\nclimb down the chair\ngrab the chair\ndrop the chair\ngrab the banana\n\n\n\nRewards\n\n-1 for each action\n+10 for grabbing the banana\n\n\n\nAdd a show code / hide code button\n\n# %load_ext autoreload\n# %autoreload 2\nimport os  # noqa\nimport sys  # noqa\n\nmodule_path = os.path.abspath(os.path.join(\"posts/monkey-banana-mdp/code\"))\nsys.path.insert(0, module_path)\nfrom environment import LineWorldEnv  # noqa\n\n\nimport numpy as np  # noqa\nimport pygame  # noqa\n\nimport gymnasium as gym  # noqa\nfrom gymnasium import spaces  # noqa\nfrom gymnasium.envs.registration import register  # noqa\n\nFirst, we test the setup by observing the Monkey take random actions.\n\nenv = LineWorldEnv(render_mode=\"human\", size=5)\nobs, info = env.reset()\n\nfor _ in range(50):\n    actions = env.get_possible_actions(env.flatten_obs(obs))\n    # Choose random action\n    action = np.random.choice(actions)\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        obs, info = env.close()\n\nThe blue circle is the Monkey, the green square is the chair, and the yellow square is the banana. On the top left you will see the action that is being taken.\n\nNext, we use Dynamic Programming with value iteration to find the optimal policy.\n\nenv = LineWorldEnv(size=10)\nobs, info = env.reset()\n\nall_states = env.get_all_states()\nstate_values = {s: 0 for s in all_states}\n\ndef action_evaluation(state, action):\n    env.set_obs(state)\n    next_state, reward, terminated, truncated, _  =  env.step(action)\n    flattened_next_state = env.flatten_obs(next_state)\n    value = reward + state_values[flattened_next_state]\n    return 0 if terminated else value\n\n# Value iteration\ntheta = 0.1\nsweep_count = 0\nbiggest_change = np.inf\nwhile biggest_change &gt; theta:\n    biggest_change = 0\n    for s in all_states:\n        original_value = state_values[s]\n        best_value = -np.inf\n        possible_actions = env.get_possible_actions(s)\n        for action in possible_actions:\n            value = action_evaluation(s, action)\n            if value &gt; best_value:\n                best_value = value\n        state_values[s] = best_value\n        biggest_change = max(biggest_change, abs(original_value - state_values[s]))\n    sweep_count += 1\nprint(\"Number of sweeps: \", sweep_count)\nenv.close()\n\n# Create optimal policy pi from state values:\npolicy = {} \nfor s in all_states:\n      possible_actions = env.get_possible_actions(s)\n      best_value = -np.inf\n      best_action = None\n      for a in possible_actions:\n          value = action_evaluation(s, a)\n          if value &gt; best_value:\n              best_value = value\n              best_action = a\n      policy[s] = best_action\n\nWe can observe the Monkey following the optimal policy produced from Dynamic Programming:\n\nenv = LineWorldEnv(render_mode=\"human\", size=10)\ncbs, info = env.reset()\n\nwhile True:\n    action = policy[env.flatten_obs(obs)]\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        env.close()\n        break"
  },
  {
    "objectID": "posts/monkey-banana-mdp/monkey-banana-dp.html",
    "href": "posts/monkey-banana-mdp/monkey-banana-dp.html",
    "title": "Solving the monkey banana problem with dynamic programming",
    "section": "",
    "text": "We will create a simplified Monkey Banana environment to test classical RL methods on.\nThe environment is a “2D world” with discrete and finite states, actions, and rewards. It is fully observable to our agent, the Monkey.\n\nState\nState consists of 5 values:\n\nagent position (x-axis)\nchair position (x-axis)\nbanana position (x-axis)\nis_holding_chair (0 or 1)\non_chair (0 or 1)\n\nAs you can see, we do not explicitly model the y-axis, since all we care about is whether the Monkey is on the chair when he reaches for the banana.\n\n\nActions\n\nmove left one step\nmove right one step\nclimb on the chair\nclimb down the chair\ngrab the chair\ndrop the chair\ngrab the banana\n\n\n\nRewards\n\n-1 for each action\n+10 for grabbing the banana\n\nFirst, we test the setup by observing the Monkey take random actions.\nThe blue circle is the Monkey, the green square is the chair, and the yellow square is the banana. On the top left you will see the action that is being taken by the Monkey.\n\nNext, we use Dynamic Programming with value iteration to find the optimal policy. After around 17 iterations, the policy converges to the optimal policy, shown below in action."
  },
  {
    "objectID": "posts/monkey-banana-mdp/monkey-banana-dp-code.html",
    "href": "posts/monkey-banana-mdp/monkey-banana-dp-code.html",
    "title": "Solving the monkey banana problem with dynamic programming",
    "section": "",
    "text": "We will create a simplified Monkey Banana environment to test classical RL methods on.\nThe environment is a “2D world” with discrete and finite states, actions, and rewards. It is fully observable to our agent, the Monkey.\n\nState\nState consists of 5 values:\n\nagent position (x-axis)\nchair position (x-axis)\nbanana position (x-axis)\nis_holding_chair (0 or 1)\non_chair (0 or 1)\n\nWe do not explicitly model the y-axis, since all we care about is whether the Monkey is on the chair when he reaches for the banana.\n\n\nActions\n\nmove left one step\nmove right one step\nclimb on the chair\nclimb down the chair\ngrab the chair\ndrop the chair\ngrab the banana\n\n\n\nRewards\n\n-1 for each action\n+10 for grabbing the banana\n\n\n# %load_ext autoreload\n# %autoreload 2\nimport os  # noqa\nimport sys  # noqa\n\nmodule_path = os.path.abspath(os.path.join(\"posts/monkey-banana-mdp/code\"))\nsys.path.insert(0, module_path)\nfrom environment import LineWorldEnv  # noqa\n\n\nimport numpy as np  # noqa\nimport pygame  # noqa\n\nimport gymnasium as gym  # noqa\nfrom gymnasium import spaces  # noqa\nfrom gymnasium.envs.registration import register  # noqa\n\nFirst, we test the setup by observing the Monkey take random actions.\n\nenv = LineWorldEnv(render_mode=\"human\", size=5)\nobs, info = env.reset()\n\nfor _ in range(50):\n    actions = env.get_possible_actions(env.flatten_obs(obs))\n    # Choose random action\n    action = np.random.choice(actions)\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        obs, info = env.close()\n\nThe blue circle is the Monkey, the green square is the chair, and the yellow square is the banana. On the top left you will see the action that is being taken.\n\nNext, we use Dynamic Programming with value iteration to find the optimal policy.\n\nenv = LineWorldEnv(size=10)\nobs, info = env.reset()\n\nall_states = env.get_all_states()\nstate_values = {s: 0 for s in all_states}\n\ndef action_evaluation(state, action):\n    env.set_obs(state)\n    next_state, reward, terminated, truncated, _  =  env.step(action)\n    flattened_next_state = env.flatten_obs(next_state)\n    value = reward + state_values[flattened_next_state]\n    return 0 if terminated else value\n\n# Value iteration\ntheta = 0.1\nsweep_count = 0\nbiggest_change = np.inf\nwhile biggest_change &gt; theta:\n    biggest_change = 0\n    for s in all_states:\n        original_value = state_values[s]\n        best_value = -np.inf\n        possible_actions = env.get_possible_actions(s)\n        for action in possible_actions:\n            value = action_evaluation(s, action)\n            if value &gt; best_value:\n                best_value = value\n        state_values[s] = best_value\n        biggest_change = max(biggest_change, abs(original_value - state_values[s]))\n    sweep_count += 1\nprint(\"Number of sweeps: \", sweep_count)\nenv.close()\n\n# Create optimal policy pi from state values:\npolicy = {} \nfor s in all_states:\n      possible_actions = env.get_possible_actions(s)\n      best_value = -np.inf\n      best_action = None\n      for a in possible_actions:\n          value = action_evaluation(s, a)\n          if value &gt; best_value:\n              best_value = value\n              best_action = a\n      policy[s] = best_action\n\nWe can observe the Monkey following the optimal policy produced from Dynamic Programming:\n\nenv = LineWorldEnv(render_mode=\"human\", size=10)\ncbs, info = env.reset()\n\nwhile True:\n    action = policy[env.flatten_obs(obs)]\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        env.close()\n        break"
  },
  {
    "objectID": "posts/monkey-banana-mdp/monkey-banana-mc.html",
    "href": "posts/monkey-banana-mdp/monkey-banana-mc.html",
    "title": "Solving the monkey banana problem with monte carlo methods",
    "section": "",
    "text": "In Monte Carlo, unlike in Dynamic Programming, we remove absolute knowledge of the environment so that the agent will learn from its own experience by undergoing repeated episodes. We use the algorithm outlined below from Sutton & Barto’s “Reinforcement Learning: An Introduction”.\n\n\n\nAlgorithm\n\n\nThe video contains a few runs of the agent completing the task. Since the policy is stochastic, we can see the agent sometimes making some non-optimal moves."
  },
  {
    "objectID": "posts/monkey-banana-mdp/monkey-banana-mc-code.html",
    "href": "posts/monkey-banana-mdp/monkey-banana-mc-code.html",
    "title": "Solving the monkey banana problem with monte carlo methods",
    "section": "",
    "text": "import os  # noqa\nimport sys  # noqa\n\nmodule_path = os.path.abspath(os.path.join(\"posts/monkey-banana-mdp/code\"))\n# module_path = os.path.abspath(os.path.join(\"./code\"))\nsys.path.insert(0, module_path)\nfrom environment import LineWorldEnv  # noqa\nfrom simple_env import SimpleLineWorldEnv  # noqa\nfrom IPython.display import Image  # noqa\nimport numpy as np  # noqa\nimport pygame  # noqa\n\nimport gymnasium as gym  # noqa\nfrom gymnasium import spaces  # noqa\n\nIn Monte Carlo, unlike in Dynamic Programming, we remove absolute knowledge of the environment so that the agent will learn from its own experience by undergoing repeated episodes. We use the algorithm outlined below from Sutton & Barto’s “Reinforcement Learning: An Introduction”.\n\nImage(filename=\"on-policy-first-visit-algo.png\")\n\n\nenv = LineWorldEnv(size=3)\n# env = LineWorldEnv(render_mode=\"human\", size=5)\nall_states = env.get_all_states()\nq_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nn = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nepsilon = 0.1\ngamma = 0.4\n\n# Epsilon-soft policy (equal probability for all actions)\npolicy = {\n    s: {a: 1 / len(env.get_possible_actions(s)) for a in env.get_possible_actions(s)}\n    for s in all_states\n}\n\n\nfor i in range(1000):\n    # for i in range(3):\n    print(f\"Episode: {i}\")\n\n    # Generate episode\n    seed = 10\n    obs, info = env.reset(seed=seed)\n    episode = []\n\n    # We start at some state. It can be the same state every time.\n    episode_len = 0\n    while True:\n        episode_len += 1\n\n        # Choose action according to stochastic policy\n        state = env.flatten_obs(obs)\n        # Each key in the policy dictionary is a state, and the value is a dictionary of actions and their probabilities\n        # Sample an action\n        action_probabilities = policy[state]\n        action = np.random.choice(\n            list(action_probabilities.keys()), p=list(action_probabilities.values())\n        )\n        # print(\"Action taken: \", action)\n\n        obs, reward, terminated, truncated, info = env.step(action)\n        episode.append((state, action, reward))\n\n        if terminated or truncated:\n            break\n\n    # Update q-value estimates based on episode generated\n    returns = 0\n    for j in range(len(episode) - 1, -1, -1):\n        state, action, reward = episode[j]\n\n        returns += gamma * returns + reward\n        exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n\n        # We only update q-value for first-visit\n        if not exists:\n            n[state][action] += 1\n            q_values[state][action] += (returns - q_values[state][action]) / n[state][\n                action\n            ]\n\n            # Update policy with the epsilon-max q-value action\n            best_action = max(q_values[state], key=q_values[state].get)\n            for action in policy[state].keys():\n                if action == best_action:\n                    policy[state][action] = 1 - epsilon + epsilon / len(policy[state])\n                else:\n                    policy[state][action] = epsilon / len(policy[state])\n\nprint(policy)\nprint(q_values)\n\n\nenv = LineWorldEnv(render_mode=\"human\", size=3)\nobs, info = env.reset()\n\nwhile True:\n    action_probabilities = policy[env.flatten_obs(obs)]\n    print(\"State: \", env.flatten_obs(obs))\n    print(\"Action probabilities: \", action_probabilities)\n    action = np.random.choice(\n        list(action_probabilities.keys()), p=list(action_probabilities.values())\n    )\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        env.close()\n        break\n\n\nenv = LineWorldEnv(size=3)\n# env = LineWorldEnv(render_mode=\"human\", size=5)\nall_states = env.get_all_states()\nq_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nn = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nepsilon = 0.1\ngamma = 0.9\n\n# Epsilon-soft policy (equal probability for all actions)\n# policy = {s: np.random.choice(env.get_possible_actions(s)) if env.get_possible_actions(s) &gt; 0 else None for s in all_states}\npolicy = {}\nfor s in all_states:\n    possible_actions = env.get_possible_actions(s)\n    if len(possible_actions) &gt; 0:  # Check if the list of possible actions is not empty\n        policy[s] = np.random.choice(possible_actions)\n    else:\n        policy[s] = None  # or some default action or handling for states with no possible actions\n\n\nfor i in range(1000):\n# for i in range(3):\n    print(f\"Episode: {i}\")\n\n    # Generate episode\n    obs, info = env.reset(seed=np.random.randint(0, 1000))\n    episode = []\n\n    # We start at some state. It can be the same state every time. \n    episode_len = 0\n    while True:\n        episode_len += 1\n\n        state = env.flatten_obs(obs)\n        action = policy[state]\n        \n        obs, reward, terminated, truncated, info = env.step(action)\n        episode.append((state, action, reward))\n\n        if terminated or truncated:\n          break                                    \n        if episode_len &gt; 100:\n          break\n    if not terminated:\n      continue\n\n\n    # Update q-value estimates based on episode generated\n    returns = 0\n    for j in range(len(episode)-1, -1, -1):\n      state, action, reward = episode[j]\n\n      returns += gamma * returns + reward\n      exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n\n      # We only update q-value for first-visit\n      if not exists:\n        n[state][action] += 1\n        q_values[state][action] += (returns - q_values[state][action]) / n[state][action]\n\n        # Update policy with the epsilon-max q-value action\n        best_action = max(q_values[state], key=q_values[state].get)\n        policy[state] = best_action\n  \nprint(policy)\nprint(q_values)\n\n\nenv = LineWorldEnv(render_mode=\"human\", size=3)\nobs, info = env.reset()\n\nwhile True:\n    print(\"State: \", env.flatten_obs(obs))\n    action = policy[env.flatten_obs(obs)]\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        env.close()\n        break\n\n\nSimple environment to debug Monte Carlo\n\nenv = SimpleLineWorldEnv(render_mode=\"human\", size=5)\nobs, info = env.reset()\n\nwhile True:\n    # Sample random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        env.close()\n        break\n\n\nenv = SimpleLineWorldEnv(size=5)\n# env = LineWorldEnv(render_mode=\"human\", size=5)\nall_states = env.get_all_states()\nq_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nn = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\nepsilon = 0.1\ngamma = 0.9\n\n# Epsilon-soft policy (equal probability for all actions)\npolicy = {s: {a: 1/len(env.get_possible_actions(s)) for a in env.get_possible_actions(s)} for s in all_states}\n\n# for i in range(1000):\nfor i in range(10):\n    print(f\"Episode: {i}\")\n\n    # Generate episode\n    seed = 10\n    obs, info = env.reset(seed=seed)\n    episode = []\n\n    # We start at some state. It can be the same state every time. \n    episode_len = 0\n    while True:\n        episode_len += 1\n\n        # Choose action according to stochastic policy\n        state = env.flatten_obs(obs)\n        # Each key in the policy dictionary is a state, and the value is a dictionary of actions and their probabilities\n        # Sample an action\n        action_probabilities = policy[state]\n        action = np.random.choice(list(action_probabilities.keys()), p=list(action_probabilities.values()))\n        # print(\"Action taken: \", action)\n        \n        obs, reward, terminated, truncated, info = env.step(action)\n        episode.append((state, action, reward))\n\n        if terminated or truncated:\n          break                                    \n\n    # Update q-value estimates based on episode generated\n    returns = 0\n    for j in range(len(episode)-1, -1, -1):\n      state, action, reward = episode[j]\n\n      returns += gamma * returns + reward\n      exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n\n      # We only update q-value for first-visit\n      if not exists:\n        n[state][action] += 1\n        q_values[state][action] += (returns - q_values[state][action]) / n[state][action]\n\n        # Update policy with the epsilon-max q-value action\n        best_action = max(q_values[state], key=q_values[state].get)\n        original_policy = policy[state].copy()\n        for action in policy[state].keys():\n          if action == best_action:\n            policy[state][action] = 1 - epsilon + epsilon / len(policy[state])\n          else:\n            policy[state][action] = epsilon / len(policy[state])\n        # Check if policy has changed\n        if original_policy[state] != policy[state]:\n              print(\"State: \", state)\n              print(\"Original state-policy\", original_policy[state])\n              print(\"New state-policy\", policy[state])\n\n          \n    print(policy)\n  \nprint(policy)\nprint(q_values)\n\n\none = {1: 0.5, 2: 0.5}\n\ntwo = {1: 0.5, 2: 0.5}\n\none == two"
  }
]