{
  "cells": [
    {
      "cell_type": "raw",
      "id": "56496daf",
      "metadata": {},
      "source": [
        "---\n",
        "title: Solving the monkey banana problem with monte carlo methods\n",
        "author: Kim Young Jin\n",
        "date: '2024-05-27'\n",
        "categories:\n",
        "  - code\n",
        "subtitle: Monte carlo code for the Monkey Banana problem\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a025da6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os  # noqa\n",
        "import sys  # noqa\n",
        "\n",
        "module_path = os.path.abspath(os.path.join(\"posts/monkey-banana-mdp/code\"))\n",
        "# module_path = os.path.abspath(os.path.join(\"./code\"))\n",
        "sys.path.insert(0, module_path)\n",
        "from environment import LineWorldEnv  # noqa\n",
        "from simple_env import SimpleLineWorldEnv  # noqa\n",
        "from IPython.display import Image  # noqa\n",
        "import numpy as np  # noqa\n",
        "import pygame  # noqa\n",
        "\n",
        "import gymnasium as gym  # noqa\n",
        "from gymnasium import spaces  # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e31feb",
      "metadata": {},
      "source": [
        "In Monte Carlo, unlike in Dynamic Programming, we remove absolute knowledge of the environment so that the agent will learn from its own experience by undergoing repeated episodes. We use the algorithm outlined below from Sutton & Barto's \"Reinforcement Learning: An Introduction\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7beda52",
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(filename=\"on-policy-first-visit-algo.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9179fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = LineWorldEnv(size=3)\n",
        "# env = LineWorldEnv(render_mode=\"human\", size=5)\n",
        "all_states = env.get_all_states()\n",
        "q_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "n = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "epsilon = 0.1\n",
        "gamma = 0.4\n",
        "\n",
        "# Epsilon-soft policy (equal probability for all actions)\n",
        "policy = {\n",
        "    s: {a: 1 / len(env.get_possible_actions(s)) for a in env.get_possible_actions(s)}\n",
        "    for s in all_states\n",
        "}\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # for i in range(3):\n",
        "    print(f\"Episode: {i}\")\n",
        "\n",
        "    # Generate episode\n",
        "    seed = 10\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    episode = []\n",
        "\n",
        "    # We start at some state. It can be the same state every time.\n",
        "    episode_len = 0\n",
        "    while True:\n",
        "        episode_len += 1\n",
        "\n",
        "        # Choose action according to stochastic policy\n",
        "        state = env.flatten_obs(obs)\n",
        "        # Each key in the policy dictionary is a state, and the value is a dictionary of actions and their probabilities\n",
        "        # Sample an action\n",
        "        action_probabilities = policy[state]\n",
        "        action = np.random.choice(\n",
        "            list(action_probabilities.keys()), p=list(action_probabilities.values())\n",
        "        )\n",
        "        # print(\"Action taken: \", action)\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    # Update q-value estimates based on episode generated\n",
        "    returns = 0\n",
        "    for j in range(len(episode) - 1, -1, -1):\n",
        "        state, action, reward = episode[j]\n",
        "\n",
        "        returns += gamma * returns + reward\n",
        "        exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n",
        "\n",
        "        # We only update q-value for first-visit\n",
        "        if not exists:\n",
        "            n[state][action] += 1\n",
        "            q_values[state][action] += (returns - q_values[state][action]) / n[state][\n",
        "                action\n",
        "            ]\n",
        "\n",
        "            # Update policy with the epsilon-max q-value action\n",
        "            best_action = max(q_values[state], key=q_values[state].get)\n",
        "            for action in policy[state].keys():\n",
        "                if action == best_action:\n",
        "                    policy[state][action] = 1 - epsilon + epsilon / len(policy[state])\n",
        "                else:\n",
        "                    policy[state][action] = epsilon / len(policy[state])\n",
        "\n",
        "print(policy)\n",
        "print(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c9d9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = LineWorldEnv(render_mode=\"human\", size=3)\n",
        "obs, info = env.reset()\n",
        "\n",
        "while True:\n",
        "    action_probabilities = policy[env.flatten_obs(obs)]\n",
        "    print(\"State: \", env.flatten_obs(obs))\n",
        "    print(\"Action probabilities: \", action_probabilities)\n",
        "    action = np.random.choice(\n",
        "        list(action_probabilities.keys()), p=list(action_probabilities.values())\n",
        "    )\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        env.close()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38051c89",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = LineWorldEnv(size=3)\n",
        "# env = LineWorldEnv(render_mode=\"human\", size=5)\n",
        "all_states = env.get_all_states()\n",
        "q_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "n = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "epsilon = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "# Epsilon-soft policy (equal probability for all actions)\n",
        "# policy = {s: np.random.choice(env.get_possible_actions(s)) if env.get_possible_actions(s) > 0 else None for s in all_states}\n",
        "policy = {}\n",
        "for s in all_states:\n",
        "    possible_actions = env.get_possible_actions(s)\n",
        "    if len(possible_actions) > 0:  # Check if the list of possible actions is not empty\n",
        "        policy[s] = np.random.choice(possible_actions)\n",
        "    else:\n",
        "        policy[s] = None  # or some default action or handling for states with no possible actions\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "# for i in range(3):\n",
        "    print(f\"Episode: {i}\")\n",
        "\n",
        "    # Generate episode\n",
        "    obs, info = env.reset(seed=np.random.randint(0, 1000))\n",
        "    episode = []\n",
        "\n",
        "    # We start at some state. It can be the same state every time. \n",
        "    episode_len = 0\n",
        "    while True:\n",
        "        episode_len += 1\n",
        "\n",
        "        state = env.flatten_obs(obs)\n",
        "        action = policy[state]\n",
        "        \n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        if terminated or truncated:\n",
        "          break                                    \n",
        "        if episode_len > 100:\n",
        "          break\n",
        "    if not terminated:\n",
        "      continue\n",
        "\n",
        "\n",
        "    # Update q-value estimates based on episode generated\n",
        "    returns = 0\n",
        "    for j in range(len(episode)-1, -1, -1):\n",
        "      state, action, reward = episode[j]\n",
        "\n",
        "      returns += gamma * returns + reward\n",
        "      exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n",
        "\n",
        "      # We only update q-value for first-visit\n",
        "      if not exists:\n",
        "        n[state][action] += 1\n",
        "        q_values[state][action] += (returns - q_values[state][action]) / n[state][action]\n",
        "\n",
        "        # Update policy with the epsilon-max q-value action\n",
        "        best_action = max(q_values[state], key=q_values[state].get)\n",
        "        policy[state] = best_action\n",
        "  \n",
        "print(policy)\n",
        "print(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd01df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = LineWorldEnv(render_mode=\"human\", size=3)\n",
        "obs, info = env.reset()\n",
        "\n",
        "while True:\n",
        "    print(\"State: \", env.flatten_obs(obs))\n",
        "    action = policy[env.flatten_obs(obs)]\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        env.close()\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54640bc1",
      "metadata": {},
      "source": [
        "### Simple environment to debug Monte Carlo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f43a2e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = SimpleLineWorldEnv(render_mode=\"human\", size=5)\n",
        "obs, info = env.reset()\n",
        "\n",
        "while True:\n",
        "    # Sample random action\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        env.close()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee3ce30",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = SimpleLineWorldEnv(size=5)\n",
        "# env = LineWorldEnv(render_mode=\"human\", size=5)\n",
        "all_states = env.get_all_states()\n",
        "q_values = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "n = {s: {a: 0 for a in env.get_possible_actions(s)} for s in all_states}\n",
        "epsilon = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "# Epsilon-soft policy (equal probability for all actions)\n",
        "policy = {s: {a: 1/len(env.get_possible_actions(s)) for a in env.get_possible_actions(s)} for s in all_states}\n",
        "\n",
        "# for i in range(1000):\n",
        "for i in range(10):\n",
        "    print(f\"Episode: {i}\")\n",
        "\n",
        "    # Generate episode\n",
        "    seed = 10\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    episode = []\n",
        "\n",
        "    # We start at some state. It can be the same state every time. \n",
        "    episode_len = 0\n",
        "    while True:\n",
        "        episode_len += 1\n",
        "\n",
        "        # Choose action according to stochastic policy\n",
        "        state = env.flatten_obs(obs)\n",
        "        # Each key in the policy dictionary is a state, and the value is a dictionary of actions and their probabilities\n",
        "        # Sample an action\n",
        "        action_probabilities = policy[state]\n",
        "        action = np.random.choice(list(action_probabilities.keys()), p=list(action_probabilities.values()))\n",
        "        # print(\"Action taken: \", action)\n",
        "        \n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        if terminated or truncated:\n",
        "          break                                    \n",
        "\n",
        "    # Update q-value estimates based on episode generated\n",
        "    returns = 0\n",
        "    for j in range(len(episode)-1, -1, -1):\n",
        "      state, action, reward = episode[j]\n",
        "\n",
        "      returns += gamma * returns + reward\n",
        "      exists = any((s, a) == (state, action) for s, a, r in episode[:j])\n",
        "\n",
        "      # We only update q-value for first-visit\n",
        "      if not exists:\n",
        "        n[state][action] += 1\n",
        "        q_values[state][action] += (returns - q_values[state][action]) / n[state][action]\n",
        "\n",
        "        # Update policy with the epsilon-max q-value action\n",
        "        best_action = max(q_values[state], key=q_values[state].get)\n",
        "        original_policy = policy[state].copy()\n",
        "        for action in policy[state].keys():\n",
        "          if action == best_action:\n",
        "            policy[state][action] = 1 - epsilon + epsilon / len(policy[state])\n",
        "          else:\n",
        "            policy[state][action] = epsilon / len(policy[state])\n",
        "        # Check if policy has changed\n",
        "        if original_policy[state] != policy[state]:\n",
        "              print(\"State: \", state)\n",
        "              print(\"Original state-policy\", original_policy[state])\n",
        "              print(\"New state-policy\", policy[state])\n",
        "\n",
        "          \n",
        "    print(policy)\n",
        "  \n",
        "print(policy)\n",
        "print(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c001d94",
      "metadata": {},
      "outputs": [],
      "source": [
        "one = {1: 0.5, 2: 0.5}\n",
        "\n",
        "two = {1: 0.5, 2: 0.5}\n",
        "\n",
        "one == two"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.2"
      }
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
